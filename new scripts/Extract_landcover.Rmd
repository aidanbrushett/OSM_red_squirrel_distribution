---
title: "0b. Extract natural composition"
author: "Aidan Brushett"
date: "2025-04-02"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
library(sf)
library(tidyverse)
```

# 1. FOREST INVENTORY DATA

## 1.1. Load, clip the forest inventory data

```{r Loading data, change the names to the current camera array/subsetted or full feature layers}
# Camera locations (point)
sites <- st_read("./maps/OSM_mapping.gdb", layer = "all_arrays_locations_covariates") %>%
  
  filter(str_detect(array, "LU")) %>%
  
  select(array, site)

plot(sites[1])

# buffer sites so that we can filter the landcover data and reduce file size
sites_buffered <- st_buffer(sites, dist = 10000) %>%
  
  st_union()

plot(sites_buffered[1])

# load landcover layer (will be slow)
landcover_unclipped <- st_read("./maps/SBFI_2020_Boreal_Plains.gdb", layer = "SBFI_2020_polygons")

landcover <- landcover_unclipped %>%
  
  st_transform(., crs = 26912) %>%
  
  filter(lengths(st_intersects(., sites_buffered)) > 0)

#make sure it's all projected correctly 
st_crs(landcover, describe = TRUE)
st_crs(sites, describe = TRUE)
plot(landcover[1])

# clean up
rm(landcover_unclipped, sites_buffered)
gc()

# double check that they are all format "sf" and "dataframe"
str(sites)
str(landcover)
```

## 1.2. Reshape data for key variables 

Tree species composition:

```{r}
tree_spp <- landcover %>%
  
  st_drop_geometry() %>%
  
  select(SPECIES_1,
         SPECIES_2,
         SPECIES_3,
         SPECIES_4,
         SPECIES_5) %>%
  
  unlist(.) %>%
  
  unique(.) %>%

  .[ . != " "] # Remove empty strings

# Re-pivot the tree data for each polygon. This can take a while and is a bit clunky. 
tree_percent <- landcover %>%
  
  st_drop_geometry() %>%
  
  select(ID,
         contains("SPECIES") & 
           !contains("_CML"),
         -SPECIES_NUMBER,
         -SPECIES_CONIFEROUS_PERC
         ) %>%
  
  mutate(across(everything(), as.character)) %>%  # Convert all columns to character
  
  pivot_longer(cols = starts_with("SPECIES_"),
               names_to = "temp",
               values_to = "val") %>%
  
  # What rank is the species? We don't actually care but this helps keep things tidy/avoids duplicates when we pivot wider later
  mutate(spp_rank = as.numeric(str_extract(temp, "\\d+"))) %>%

  # Remove the numbers so that all the former rank columns are just called "species"
  mutate(temp = str_replace_all(temp, "_\\d+", ""),
         temp = str_replace_all(temp, "SPECIES_PERC", "PERCENTAGE")) %>%
  
  # Pivot wider so we now have a species column and a percent column
  pivot_wider(names_from = temp, values_from = val) %>%
  
  # Only want the non-zero percents
  filter(PERCENTAGE > 0) %>%

  # Pivot wider again
  pivot_wider(names_from = SPECIES, values_from = PERCENTAGE, values_fill = "") %>%
  
  select(-spp_rank) %>%
  
  mutate(across(everything(), as.numeric)) %>%
  
  mutate(across(everything(), ~ replace(., is.na(.), 0))) %>%

  group_by(ID) %>%

  summarize(across(everything(), sum))
```

Percent harvest by year:

```{r}
harvest_percent <- landcover %>% 
  
  st_drop_geometry(.) %>%
  
  select(ID,
         contains("DISTURB_HARVEST")) %>%
  
  pivot_wider(names_from = DISTURB_HARVEST_YEAR, 
              values_from = DISTURB_HARVEST_PERC, 
              values_fill = 0) %>%
  
  select(-`0`) %>%
  
  rename_with(., ~paste0("HARVEST_PCT_", .), .cols = !contains("ID")) %>%
    
  select(ID, sort(names(.)))
```

Percent burn by year:

```{r}
fire_percent <- landcover %>% 
  
  st_drop_geometry(.) %>%
  
  select(ID,
         DISTURB_FIRE_YEAR,
         DISTURB_FIRE_PERC) %>%
  
  pivot_wider(names_from = DISTURB_FIRE_YEAR, 
              values_from = DISTURB_FIRE_PERC, 
              values_fill = 0) %>%
  
  select(-`0`) %>%
  
  rename_with(., ~paste0("FIRE_PCT_", .), .cols = !contains("ID")) %>%
    
  select(ID, sort(names(.)))
```

Intensity of fire for a given polygon:

```{r}
fire_intensity <- landcover %>% 
  
  st_drop_geometry(.) %>%
  
  select(ID,
         DISTURB_FIRE_MAGNITUDE_AVG,
         DISTURB_FIRE_YEAR) %>%
  
  pivot_wider(names_from = DISTURB_FIRE_YEAR, 
              values_from = DISTURB_FIRE_MAGNITUDE_AVG, 
              values_fill = 0) %>%
  
  select(-`0`) %>%
  
  rename_with(., ~paste0("FIRE_MAG_", .), .cols = !contains("ID")) %>%
    
  select(ID, sort(names(.)))


landcover_recalculated <- landcover %>%
  
  left_join(tree_percent, by = "ID") %>%
  
  left_join(fire_percent, by = "ID") %>%
  
  left_join(fire_intensity, by = "ID") %>%
  
  left_join(harvest_percent, by = "ID") %>%
  
  mutate(across(
        .cols = c(all_of(tree_spp), contains("FIRE_PCT"), contains("HARVEST_PCT")), 
        ~ replace(., is.na(.), 0))) %>%
  
  rename_with(., ~paste0(., "_PCT_OF_TREED"), .cols = all_of(tree_spp))

rm(tree_spp, tree_percent, fire_percent, fire_intensity, harvest_percent)
```

## 1.3. Identify variables to extract

```{r Extract covariates for forest age, message=FALSE, warning=FALSE}
landcover_metrics <- landcover_recalculated %>% 
  
  # Select the metrics that you want to calculate
  select(., 
         
         # Forest age
         contains("AGE_"),
         -AGE_AVG,
         -AGE_MIN,
         -AGE_MAX,
         -AGE_MEDIAN,
         -AGE_SD,
         
         # Landcover
         contains("LC_"),
         -LC_FAO_FOREST, 
         -LC_TREED, 
         -LC_WETLAND_VEGETATION,
         
         # Tree composition
         contains("_PCT_OF_TREED"),
         
         # Fire and harvest percent
         contains("FIRE_PCT"),
         contains("HARVEST_PCT")
         )
```

## 1.4. Extract data for the sites of interest

This sums the % cover within each polygon and is weighted by their proportion of area within the buffer.

-   e.g. a polygon takes up 0.50 of the buffer and was 50% conifer = 25% conifer total
-   an additional polygon takes up 0.25 of the buffer and was 80% conifer = 20% conifer total
-   the proportion cover for the total buffer around the site is 45% conifer. 
-   *within a polygon the vegetation attributes are assumed to be constant*

```{r Extract covariates for forest age, message=FALSE, warning=FALSE}
sites_landcover_metrics <- list()

start_time <- Sys.time()

for(i in 1:nrow(sites)) {
  
  progress <- (i - 1) / nrow(sites)
    
  site <- sites[i,]

  cat("\r Extracting site ", i, " of ", nrow(sites), " (", site$site, "): ", 
      round(as.numeric(difftime(Sys.time(), start_time, units = "hours")) * (1 - progress) / progress, 2), 
      " hours remaining        ", sep = "")

  sites_landcover_metrics[[i]] <- 

    purrr::map_dfr(seq(250, 5000, by = 250), ~
                     
      st_intersection(landcover_metrics,
                      st_buffer(site, .x)) %>%
      
      mutate(feature_area = st_area(.) %>% 
               as.numeric(.),
             total_area = sum(feature_area, na.rm = TRUE) %>% 
               as.numeric(.),
             percent_area = feature_area / total_area,
             across(where(is.numeric) & !contains("_area"), ~ . / 100 * percent_area)) %>%
  
      st_drop_geometry(.) %>%
    
      summarise(across(where(is.numeric) & !contains("_area"), sum, na.rm = TRUE)) %>%
    
      mutate(buffer_dist = .x,
             array = site$array,
             site = site$site)
      
    )
  
  rm(site)
  gc()

}
```

## 1.5. Save results

```{r Extract covariates for forest age, message=FALSE, warning=FALSE}
save(sites_landcover_metrics, file = "./data/raw/OSM_SBFI2020_metrics.RData")

sites_landcover_metrics_df <- bind_rows(sites_landcover_metrics) %>%

  relocate(array, site, buffer_dist)

write_csv(sites_landcover_metrics_df, "./data/raw/OSM_SBFI2020_metrics.csv")
```


***


# 2. HUMAN FOOTPRINT DATA

## 2.1. Import data

```{r Loading data, change the names to the current camera array/subsetted}
# Camera locations (point)
sites <- st_read("./maps/OSM_mapping.gdb", layer = "all_arrays_locations_covariates") %>%
  
  filter(str_detect(array, "LU")) %>%
  
  select(array, site)

plot(sites[1])

# buffer sites so that we can filter the landcover data and reduce file size
sites_buffered <- st_buffer(sites, dist = 10000) %>%
  
  st_union()

plot(sites_buffered[1])

# Human footprint index:
# load human features layer (will be slow)
st_layers("./maps/HFI2021_clipped.gdb")

hfi <- st_read("./maps/HFI_clipped/HFI_2021_clipped.shp") %>%
  
  st_transform(., crs = 26912)


#make sure it's all projected correctly 
st_crs(hfi, describe = TRUE)
st_crs(sites, describe = TRUE)
plot(hfi[1])

# clean up
rm(sites_buffered)
gc()

# double check that they are all format "sf" and "dataframe"
str(sites)
str(hfi)
```

## 2.2. Extract variables

```{r message=FALSE, warning=FALSE}
hfi_metrics <- hfi %>%
  
  select(OBJECTID, FEATURE_TY, YEAR)

# List to store results
sites_hfi_metrics <- list()

start_time <- Sys.time()

for(i in 1:nrow(sites)) {
  
  progress <- (i - 1) / nrow(sites)
    
  site <- sites[i,]

  cat("\r Extracting site ", i, " of ", nrow(sites), " (", site$site, "): ", 
      round(as.numeric(difftime(Sys.time(), start_time, units = "hours")) * (1 - progress) / progress, 2), 
      " hours remaining        ", sep = "")

  sites_hfi_metrics[[i]] <- 

    purrr::map_dfr(seq(250, 5000, by = 250), ~{
      
      site_buffer <- st_buffer(site, .x)           

      st_intersection(hfi_metrics,
                      site_buffer) %>%

      mutate(feature_area = st_area(.) %>%
               as.numeric(.),
             total_area = st_area(site_buffer) %>% 
               as.numeric(.),
             percent_area = feature_area / total_area) %>%
 
      st_drop_geometry(.) %>%
   
      mutate(FEATURE_TY = ifelse(FEATURE_TY=="HARVEST-AREA", paste0(FEATURE_TY, "-", YEAR), FEATURE_TY)) %>%
   
      group_by(array, site, FEATURE_TY) %>%
    
      summarize(PCT_COVER = sum(percent_area),
                buffer_dist = .x) %>%
   
      pivot_wider(names_from = FEATURE_TY, 
                  values_from = PCT_COVER, 
                  values_fill = 0)
    }
    )
  
  rm(site)
  gc()

}
```

## 2.3. Save results

```{r message=FALSE}
save(sites_hfi_metrics, file = "./data/raw/OSM_HFI2021_metrics.RData")

sites_hfi_metrics_df <- bind_rows(sites_hfi_metrics) %>%
  
  ungroup() %>%

  relocate(array, site, buffer_dist, sort(names(.))) %>%
  
  select(-array) %>%
  
  complete(site, buffer_dist) %>%
  
              # Create some extra columns
  separate_wider_delim(site,
                       delim = '_',
                       names = c('array', 'camera'),
                       cols_remove = FALSE) %>%
  
  select(-camera) %>%
  
  mutate(across(4:last_col(), ~replace(., is.na(.), 0)))

write_csv(sites_hfi_metrics_df, "./data/raw/OSM_HFI2021_metrics.csv")
```


***


# 3. CONFIGURATION METRICS

## 3.1. Load additional packages

```{r}
library(tidyverse)
library(terra)
library(sf)
library(landscapemetrics)
```

## 3.2. Import the rasters and other data

Rasters we want: 

```{r}
configuration_simple <- rast("./rasters/OSM_landcover_HFI_binary.tif")
plot(configuration_simple)

#configuration_simplified <- rast("./rasters/OSM_HFI_SBFI_forested_classes.tif")
#plot(configuration_simplified)

configuration_grouped <- rast("./rasters/OSM_HFI_SBFI_all_veg_classes.tif")
plot(configuration_grouped)
```

Classification schemes that tell us which landcover type each raster value corresponds to (I made this manually while I was processing the rasters):

```{r}
features_grouped <- read_csv("./rasters/OSM_HFI_SBFI_raster_feature_types.csv") %>%
  
  select(feature = name.OSM_HFI_SBFI_all_veg_classes, class = value.OSM_HFI_SBFI_all_veg_classes) %>%
  
  mutate(class = as.character(class)) %>%
  
  distinct()

#features_simplified <- read_csv("./rasters/OSM_landcover_HFI_feature_types.csv") %>%
#  
#  select(feature = feature_simplified, class = value_simplified) %>%
#  
#  mutate(class = as.character(class)) %>%
#  
#  distinct()
#
#plot(landscape)
#check_landscape(configuration)
```

The sites for which we are going to extract covariates:

```{r}
sites <- st_read("./maps/OSM_mapping.gdb", layer = "all_arrays_locations_covariates") %>%
  
  filter(str_detect(array, "LU")) %>%
  
  select(array, site)

plot(sites[1])
```

## 3.4. Extract values from the "simple" landscape raster

To do this, we will represent the landscape as a simplified raster of human and non-human landcover. We'll extract:

-   **mean core area index of natural habitat** as a representation of disturbance relief (is most habitat edge or core?). Measures mean ratio of core to edge habitat in patches (CAI = 0 when the patch has no core area and approaches CAI = 100 with increasing percentage of core area within a patch.)
-   **edge density of natural habitat** (as a representation of edge *created by* interfaces with human disturbances)
-   **total core area of natural habitat** -- amount of natural habitat that is > 50 m from edges

```{r Extract landscapemetrics - raster w/ SIMPLE landcover classes}
# Create a buffer of 5500m around the site
# Convert the buffer to a SpatVector for use with terra
# Extract the raster values within the buffer

config_data <- list()

start_time <- Sys.time()

for(i in 1:nrow(sites)){
  
  site <- sites[i,]
  
  config_local <- crop(configuration_simple,
                     vect(st_buffer(site, 
                                    dist = 5100)))
  
  #plot(config_local)
  
  config_data_lu <- list()

  for (size in seq(250, 5000, by = 250)) {
    
    progress <- ((i - 1) * 20 + size/250) / (20*nrow(sites))

    cat("\r Extracting site", i, "of", nrow(sites), "(", site$site, "//", size, "m buffer ):", round(as.numeric(difftime(Sys.time(), start_time, units = "hours")), 4) * (1 - progress) / progress, "hours remaining    ")
    
    config_data_lu[[as.character(size)]] <- sample_lsm(config_local, y = site, 
             # Create the buffer and sizes of buffer (we made a loop for size above)
             shape = "circle", size = size, 
             # Calculations we would like to extract
             what = c(
               "lsm_c_ed",
               "lsm_c_cai_mn",
               "lsm_c_tca"
               ), 
             # Directions set to queen's case (8)
             directions = 8,
             # Number of pixels considered to be edge
             edge_depth = 10, # a 50 meter edge depth, in line with literature
             # If cells ONLY neighbour the landscape boundary, count as core
             consider_boundary = TRUE, 
             # Returns NA for classes not present in sample plots
             all_classes = TRUE,
             # Matches up the buffer with the site, very important
             plot_id = site$site,
             # Print warning messages
             verbose = TRUE, 
             # Print progress report
             progress = FALSE,
             # Do not return the clipped raster
             return_raster = FALSE) %>%
  
        mutate(buffer = size,
             class = as.character(class) %>% replace_na(., "landscape"),
             class = case_when(class == "0" ~ "water",
                               class == "1" ~ "natural",
                               class == "2" ~ "anthropogenic",
                               TRUE ~ class)
                               ) %>%
      
      select(-id, -layer)
    
    gc()
    
  }
  
  config_data[[site$site]] <- bind_rows(config_data_lu)
  
}

save(config_data, file = "./data/raw/OSM_simple_config_landscapemetrics.RData")
```

Manipulate the data into tidy format and save the results:

```{r}
load("./data/raw/OSM_simple_config_landscapemetrics.RData")

config_SIMPLE <- config_data %>%
  
  bind_rows(.) %>%
  
  filter(!(level == 'class' & class == "LANDSCAPE")) %>%
  
  mutate(covariate = paste0(tolower(class), "_", metric),
         site = plot_id) %>%
  
  select(-metric, -level, -class, -percentage_inside, -plot_id) %>%
  
  pivot_wider(names_from = covariate,
              values_from = value)

write_csv(config_simple, "./data/raw/OSM_simple_config_landscapemetrics.csv")
```

## 3.5. Extract landscape covariates for the FULL CLASSES ('all_veg_classes' i.e. 'grouped') landcover raster

We will extract:

-   **landscape-scale number of patches** as a measure of patch size -- is the landscape a few large patches or many small ones? *This will need to be standardized by area post-hoc*
-   **landscape-scale cohesion** -- a measure of the probability two points of a class are connected. Similar to mesh size but considers edge lengths too. So a perforated forest that's still one patch will go down. Influenced by class-level cohesion & weighted by dominance
-   **landscape-scale mesh size** -- the probability of two pixels on the landscape being in the same patch
-   **landscape scale shannon evenness index** -- a relative measure of dominance
-   **landscape scale simpson evenness index** -- a relative measure of dominance that is less sensitive to rare species

```{r Extract landscapemetrics - raster w/ OSM grouped landcover classes}

config_data <- list()

start_time <- Sys.time()

for(i in 1:nrow(sites)){
  
  site <- sites[i,]
  
  # Create a buffer of 5500m around the site
  # Convert the buffer to a SpatVector for use with terra
  # Extract the raster values within the buffer
  config_local <- crop(configuration_grouped,
                     vect(st_buffer(site, 
                                    dist = 5500)))
  
  #plot(config_local)
  
  config_data_lu <- list()

  for (size in seq(250, 5000, by = 250)) {
    
    progress <- ((i - 1) * 20 + size/250) / (20*nrow(sites))

    cat("\r Extracting site", i, "of", nrow(sites), "(", site$site, "//", size, "m buffer ):", round(as.numeric(difftime(Sys.time(), start_time, units = "hours")), 4) * (1 - progress) / progress, "hours remaining    ")
    
    config_data_lu[[as.character(size)]] <- sample_lsm(config_local, y = site, 
             # Create the buffer and sizes of buffer (we made a loop for size above)
             shape = "circle", size = size, 
             # Calculations we would like to extract
             what = c(
               "lsm_l_np",
               "lsm_l_cohesion",
               "lsm_l_mesh",
               "lsm_l_shei",
               "lsm_l_siei"
               ), 
             # Directions set to queen's case (8)
             directions = 8,
             # Number of pixels considered to be edge
             edge_depth = 10, # a 50 meter edge depth, in line with literature
             # If cells ONLY neighbour the landscape boundary, count as core
             consider_boundary = TRUE, 
             # Returns NA for classes not present in sample plots
             all_classes = TRUE,
             # Matches up the buffer with the site, very important
             plot_id = site$site,
             # Print warning messages
             verbose = TRUE, 
             # Print progress report
             progress = FALSE,
             # Do not return the clipped raster
             return_raster = FALSE) %>%
  
        mutate(buffer = size) %>%
      
      select(-id, -layer)
    
    gc()
    
  }
  
  config_data[[site$site]] <- bind_rows(config_data_lu)
  
}

save(config_data, file = "./data/raw/OSM_grouped_config_landscapemetrics.RData")
```

Manipulate the data into tidy format and save the results:

```{r}
load("./data/raw/OSM_grouped_config_landscapemetrics.RData")

config_grouped <- config_data %>%
  
  bind_rows(.) %>%
  
  # landscape scale metrics (e.g. splitting index have an NA that we want to replace)
  mutate(class = ifelse(level == "landscape", "landscape", as.character(class))) %>%
  
  drop_na(class) %>%
  
  left_join(features_grouped, by = 'class') %>%
  
  mutate(class = tolower(feature) %>%
           replace_na(., "landscape")) %>%
  
  mutate(covariate = paste0(tolower(class), "_", metric),
         site = plot_id) %>%
  
  select(-metric, -level, -class, -percentage_inside, -plot_id, -feature) %>%
  
  pivot_wider(names_from = covariate,
              values_from = value)

write_csv(config_grouped, "./data/raw/OSM_grouped_config_landscapemetrics.csv")
```
